{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>An example of SNS</center>\n",
    "\n",
    "- Threadless.com is a crowdsouring website for graphic designs.\n",
    "- Desginers submit artworks and recieve ratings from the community within a seven-day period. \n",
    "- Designs with the best scores will be selected to print on T-shirts and other products for sale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping objectives\n",
    "\n",
    "- Get a sample of users and artifacts. Consider a sampling strategy. \n",
    "- Scrape artifact-level features.\n",
    "- Scrape user-level features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.threadless.com/designs/archive?page=1', 'https://www.threadless.com/designs/archive?page=2', 'https://www.threadless.com/designs/archive?page=3', 'https://www.threadless.com/designs/archive?page=4', 'https://www.threadless.com/designs/archive?page=5']\n"
     ]
    }
   ],
   "source": [
    "# Get five urls of pages as a sample of latest artifacts.\n",
    "\n",
    "link=\"https://www.threadless.com/designs/archive?page=\"\n",
    "num=list(range(1,6))\n",
    "pages=[]\n",
    "for i in num:\n",
    "    page=link+str(i)\n",
    "    pages.append(page)\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on page https://www.threadless.com/designs/archive?page=1\n",
      "working on page https://www.threadless.com/designs/archive?page=2\n",
      "working on page https://www.threadless.com/designs/archive?page=3\n",
      "working on page https://www.threadless.com/designs/archive?page=4\n",
      "working on page https://www.threadless.com/designs/archive?page=5\n"
     ]
    }
   ],
   "source": [
    "# Get urls of all the designs in these pages\n",
    "# To reduce the load to their server, will demonnstrate one page\n",
    "\n",
    "designs=[]\n",
    "for i in pages:\n",
    "    print('working on page'+str(' ')+str(i))\n",
    "    response=requests.get(i)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "    links=soup.find('ol',class_='feed-archive th-grided')\n",
    "    li=links.find_all('li',class_=\"old\")\n",
    "    for j in li:\n",
    "        name=j.find(\"a\")[\"href\"]\n",
    "        designs.append(name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/designs/the-robot-ii',\n",
       " '/designs/betta-fish-blue',\n",
       " '/designs/lips-and-lashes-red',\n",
       " '/designs/lips-and-lashes-pink',\n",
       " '/designs/appendix-surgery-design-cute-appendectomy-two-thum']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "designs[:5]\n",
    "\n",
    "# can write out the sample of artifacts \n",
    "# with open('designs.csv', 'w') as csvfile:\n",
    "#    writer=csv.writer(csvfile, delimiter=',')\n",
    "#    writer.writerows(zip(designs))\n",
    "\n",
    "\n",
    "# read in your sample\n",
    "# raw_data_file = open(\"designs.csv\", 'r')\n",
    "# csv_data_file = csv.reader(raw_data_file, delimiter=',')\n",
    "# designs = []\n",
    "# for line in csv_data_file:\n",
    "#     print(line[0])\n",
    "#     designs.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Robot II', 'Kaiser_Paul', '3.43', '7')\n",
      "('Betta Fish Blue', 'JulieErinDesign', '3.40', '5')\n",
      "('Lips and Lashes Red', 'JulieErinDesign', '4.00', '1')\n",
      "('Lips and Lashes Pink', 'JulieErinDesign', '3.00', '2')\n",
      "('Appendix Surgery Design - Cute Appendectomy Two Thumbs Up Appendicitis Humor', 'iheartguts', '3.00', '2')\n",
      "('Sherlock Bones', 'rachelpilmoor', '4.18', '17')\n",
      "('Lucky Liver - Cute Liver Surgery Transplant Funny Cirrhosis Hepatitis Humor', 'iheartguts', '2.50', '2')\n",
      "('Sherlock Bones', 'rachelpilmoor', '4.42', '19')\n",
      "('Sherlock Bones', 'rachelpilmoor', '4.50', '10')\n",
      "(\"Crappy Valentine's Day\", 'Babsadee', '3.86', '14')\n",
      "('Lepidoptera Trio No. 2', 'CourtneyKMann', '3.50', '2')\n",
      "('Big Brain Energy', 'iheartguts', '3.50', '2')\n",
      "('Recycle Your Head (Reversed)', 'writerlayne', '3.25', '4')\n",
      "('Love Your Brain', 'iheartguts', '2.00', '2')\n",
      "('Bigfoot', 'portokalis', '3.80', '5')\n",
      "('Bigfoot', 'portokalis', '5.00', '1')\n",
      "('Types of Ta-Tas - Cute Breasts All Shapes Colors Sizes Funny Mammogram Cancer Awareness', 'iheartguts', '3.00', '3')\n",
      "('Amor', 'KreativK', '4.00', '2')\n",
      "('Bottle Kilns, Stoke on Trent', 'GWART', '4.00', '1')\n",
      "('Lazer Eyeballs', 'iheartguts', '3.00', '2')\n",
      "('The Antisocial', 'ccelestec', '2.50', '2')\n",
      "('Page Me Baby!', 'shoppeser', '2.50', '6')\n",
      "('Kestrel 1', 'palaemon', '3.00', '2')\n",
      "('Scary Green Creature With Piercing Look', 'VaporwaveAI', '2.33', '6')\n",
      "('Spooky Alien', 'VaporwaveAI', '2.67', '6')\n",
      "('Brownie Pug', 'GoldenHeavens', '4.67', '3')\n",
      "('Mummy skull', 'Psychoslime', '2.89', '9')\n",
      "('Kind of a Va-genius Funny Cute OBGYN Health Sex Ed', 'iheartguts', '2.00', '2')\n",
      "('Rainbow Kitty', 'Mckennaii', '3.83', '24')\n",
      "('Mushroom king', 'Isaiahfx1420', '3.00', '2')\n",
      "('Pterx Pride', 'Queerandcreate', '2.60', '5')\n",
      "('Pterx Pride', 'Queerandcreate', '1.00', '2')\n",
      "('Valentina - Arrive in style - Red', 'dandesign70', '2.50', '2')\n",
      "('Valentina - Arrive in style - Green', 'dandesign70', '2.00', '2')\n",
      "('Valentina - Arrive in style - Yellow', 'dandesign70', '3.50', '4')\n",
      "('Valentina - Arrive in style - Blue', 'dandesign70', '2.50', '2')\n",
      "('Happy pride guy', 'Isaiahfx1420', '1.67', '3')\n",
      "('Pennywise', 'Eclatbyjasline', '3.50', '6')\n",
      "('Happy Love Frog', 'lacychenault', '3.45', '11')\n",
      "('Type-checking like a boss: The Typescript Enforcer', 'hitechmom', '2.80', '5')\n"
     ]
    }
   ],
   "source": [
    "# Get artifact level features\n",
    "# For each design, get title, author, average score, number of scores, challenge name\n",
    "\n",
    "rows=[]\n",
    "\n",
    "for i in designs[:40]:\n",
    "    try:\n",
    "        url=\"https://www.threadless.com\"+i\n",
    "        response=requests.get(url)\n",
    "        soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # initiate the variable for each period\n",
    "        title=None\n",
    "        author=None\n",
    "        avg_score=None\n",
    "        total_score=None\n",
    "        \n",
    "        ##title\n",
    "        title=soup.select('div.submission-title h1')\n",
    "        if title!=[]:\n",
    "            title=title[0].text\n",
    "\n",
    "        ##author\n",
    "        author=soup.select('div.author-block a.author')\n",
    "        if author!=[]:\n",
    "            author=author[0].text\n",
    "\n",
    "        ##score\n",
    "        avg_score=soup.select('div.vote-avg span')\n",
    "        if avg_score!=[]:\n",
    "            avg_score=avg_score[0].text\n",
    "\n",
    "        ##total scores\n",
    "        total_score=soup.select('div.vote-count span')\n",
    "        if total_score!=[]:\n",
    "            total_score=total_score[0].text\n",
    "        \n",
    "        rows.append((title, author, avg_score, total_score))\n",
    "        print((title, author, avg_score, total_score))\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threadless\n",
      " 131611 designs\n",
      "\n",
      "Shoes\n",
      " 4267 designs\n",
      "\n",
      "Threadless\n",
      " 131611 designs\n",
      "\n",
      "Y2K\n",
      " 692 designs\n",
      "\n",
      "Horror\n",
      " 5452 designs\n"
     ]
    }
   ],
   "source": [
    "# Question: How to scrape the challenge information?\n",
    "\n",
    "# 1. challenge name\n",
    "# 2. how many designs per challenge\n",
    "\n",
    "\n",
    "# add your code here\n",
    "\n",
    "for i in designs[-5:]:\n",
    "    \n",
    "    url=\"https://www.threadless.com\"+i\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "    challenge=soup.find(\"article\",class_=\"about-the-challenge\")\n",
    "    title=challenge.select(\"li.challenge-title\")[0].text\n",
    "    num=challenge.select(\"i.fa-thumbs-up\")[0].next_sibling.next_sibling.text\n",
    "    print(title, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['portokalis', 'dandesign70', 'KreativK', 'ccelestec', 'hitechmom', 'iheartguts', 'JulieErinDesign', 'Queerandcreate', 'writerlayne', 'rachelpilmoor', 'VaporwaveAI', 'Mckennaii', 'CourtneyKMann', 'GoldenHeavens', 'shoppeser', 'Psychoslime', 'Babsadee', 'palaemon', 'Eclatbyjasline', 'GWART', 'Kaiser_Paul', 'lacychenault', 'Isaiahfx1420']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get authors\n",
    "authors=[row[1] for row in rows]\n",
    "authors=filter(None, authors)\n",
    "authors_unique=list(set(authors))\n",
    "print(authors_unique)\n",
    "len(authors_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, '50 designs submitted', '876 designs scored', 'Avg Score Given: 2.08', 'Member since 2018', 'rachelpilmoor']\n",
      "['1 thread started', '54 designs submitted', '1,151 designs scored', 'Avg Score Given: 2.99', 'Member since 2016', 'Babsadee']\n",
      "[None, '14 designs submitted', None, 'Avg Score Given: 0.00', 'Member since 2016', 'iheartguts']\n",
      "[None, '13 designs submitted', '11 designs scored', 'Avg Score Given: 5.00', 'Member since 2018', 'JulieErinDesign']\n",
      "[None, '18 designs submitted', '734 designs scored', 'Avg Score Given: 2.70', 'Member since 2023', 'Kaiser_Paul']\n"
     ]
    }
   ],
   "source": [
    "# For the designers we found, get the summary of their experience\n",
    "full=[]\n",
    "\n",
    "for i in authors_unique[:5]:\n",
    "    url=\"https://www.threadless.com/@\"+i\n",
    "    time.sleep(5)\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # find all stats\n",
    "    stats=soup.select('div.stats ul')\n",
    "    li=stats[0].find_all('li')\n",
    "    \n",
    "    line=[None] * 5\n",
    "    for j in li:\n",
    "        char=(j.text).strip()\n",
    "        \n",
    "        # threads\n",
    "        if re.search(\"started\",char):\n",
    "            line[0]=char\n",
    "            #line[1]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "            \n",
    "        # submitted\n",
    "        if re.search(\"submitted\",char):\n",
    "            line[1]=char   \n",
    "            #line[1]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "\n",
    "        # scored\n",
    "        if re.search(\"scored\",char):\n",
    "            line[2]=char\n",
    "            #line[2]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "        \n",
    "        # given\n",
    "        if re.search(\"Given\",char):\n",
    "            line[3]=char\n",
    "            #line[3]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "\n",
    "        # since\n",
    "        if re.search(\"since\",char):\n",
    "            line[4]=char\n",
    "            #line[4]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "    \n",
    "    line.append(i)\n",
    "    print(line)\n",
    "    full.append(line)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rachelpilmoor 63 82\n",
      "Babsadee 23 315\n",
      "iheartguts 0 248\n",
      "JulieErinDesign 23 17\n",
      "Kaiser_Paul 1 20\n"
     ]
    }
   ],
   "source": [
    "# Question: how to scrape each designers' numbers of followers and following?\n",
    "\n",
    "\n",
    "\n",
    "# add you code here\n",
    "for i in authors_unique[:5]:\n",
    "    url=\"https://www.threadless.com/@\"+i\n",
    "    time.sleep(5)\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    \n",
    "    # get the section\n",
    "    follow=soup.select(\"div.following li\")\n",
    "    following=follow[0].select(\"a span\")[0].text\n",
    "    follower=follow[1].select(\"a span\")[0].text\n",
    "\n",
    "    print(i, following, follower)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the follower-followee network for each designer.\n",
    "# Can we do this with beautifulsoup? \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queerandcreate', 'lacychenault']\n",
      "['Queerandcreate', 'rachelpilmoor']\n",
      "['Queerandcreate', 'rachelpilmoor']\n"
     ]
    }
   ],
   "source": [
    "relations=[]\n",
    "\n",
    "for i in authors_unique[1:10]:\n",
    "    \n",
    "    i=i.replace(\" \",\"%20\")\n",
    "    \n",
    "    follower_url=\"https://www.threadless.com/@\"+i+\"/followers\"\n",
    "    following_url=\"https://www.threadless.com/@\"+i+\"/following\"\n",
    "\n",
    "    # close a pop ad\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"user-agent=gene\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    # one's follower   \n",
    "    driver.get(follower_url)  \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # you can scroll many times if not reaching the end\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  \n",
    "    time.sleep(10)        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html.encode('utf-8'),\"html.parser\")\n",
    "    comp=soup.find('ol',class_=\"following-list\")\n",
    "    comp=comp.find_all(\"li\")\n",
    "\n",
    "    line=[]\n",
    "    for k in comp:\n",
    "        name=k.find(\"a\")[\"href\"]\n",
    "        name=name.lstrip(\"/@\")\n",
    "        if name in authors_unique:\n",
    "            # one's followers send the following tie\n",
    "            line=[name, i]\n",
    "            print(line)\n",
    "            relations.append(line)\n",
    "    \n",
    "    # one's follwing\n",
    "    driver.get(following_url)\n",
    "    time.sleep(10)   \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")                \n",
    "    time.sleep(25)  \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html.encode('utf-8'),\"html.parser\")\n",
    "    comp=soup.find('ol',class_=\"following-list\")\n",
    "    comp=comp.find_all(\"li\")\n",
    "\n",
    "    line=[]\n",
    "    for k in comp:\n",
    "        name=k.find(\"a\")[\"href\"]\n",
    "        name=name.lstrip(\"/@\")\n",
    "        if name in authors_unique:\n",
    "            # one sends the following tie to those to follow\n",
    "            line=[i, name]\n",
    "            print(line)\n",
    "            relations.append(line)\n",
    "    driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
