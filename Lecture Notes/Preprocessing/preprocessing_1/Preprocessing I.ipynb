{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/jingyisun/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "<img src=\"text_mining.png\">\n",
    "\n",
    " - Some definitions\n",
    "   * Corpus: a collection of documents\n",
    "   * Documents, e.g., articles, speeches, tweets...\n",
    "   * Documents' metadata, e.g., author, date, language...\n",
    " - Preprocessing objectives:\n",
    "   * Split documents into tokens, phrases, or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic preprocessing steps:\n",
    "   * Tokenization: split documents into individual words, phrases, or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Document-to-term matrix \n",
    " - NLP packages: NLTK, Gensim, spaCy\n",
    " - Inherent assumption-\"Bag of Words\"!\n",
    "   * We assume word order doesn't matter.\n",
    "   * \"A simple list of words, which we call unigrams, is often sufficient to convey the general meaning of a text\" (Grimmer & Stewart, 2010).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re    # import re module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The FDA's setting a minimum recommendation for efficacy doesn't mean vaccines couldn't perform better. The benchmark is also a reminder that COVID-19 vaccine development is in its early days. If the first vaccines made available only meet the minimum, they may be replaced by others that prove to protect more people. But with more than 1 million deaths from COVID-19 worldwide — and U.S. deaths surpassing 200,000 — the urgency in finding a vaccine that safely helps at least some people is at the forefront.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this extract is from https://www.sciencenews.org/article/coronavirus-what-does-covid-19-vaccine-efficacy-mean\n",
    "\n",
    "text = \"The FDA's setting a minimum recommendation for efficacy doesn't mean vaccines \\\n",
    "couldn't perform better. The benchmark is also a reminder that COVID-19 vaccine \\\n",
    "development is in its early days. If the first vaccines made available only meet \\\n",
    "the minimum, they may be replaced by others that prove to protect more people. \\\n",
    "But with more than 1 million deaths from COVID-19 worldwide — \\\n",
    "and U.S. deaths surpassing 200,000 — the urgency in finding a \\\n",
    "vaccine that safely helps at least some people is at the forefront.\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. \n",
    " - Vocabularies are the set of tokens that occur in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "['The', 'FDA', 's', 'setting', 'a', 'minimum', 'recommendation', 'for', 'efficacy', 'doesn', 't', 'mean', 'vaccines', 'couldn', 't', 'perform', 'better', 'The', 'benchmark', 'is', 'also', 'a', 'reminder', 'that', 'COVID', '19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', 'But', 'with', 'more', 'than', '1', 'million', 'deaths', 'from', 'COVID', '19', 'worldwide', 'and', 'U', 'S', 'deaths', 'surpassing', '200', '000', 'the', 'urgency', 'in', 'finding', 'a', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'FDA',\n",
       " 's',\n",
       " 'setting',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'recommendation',\n",
       " 'for',\n",
       " 'efficacy',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'mean',\n",
       " 'vaccines',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'perform',\n",
       " 'better',\n",
       " 'The',\n",
       " 'benchmark',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'reminder',\n",
       " 'that',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'development',\n",
       " 'is',\n",
       " 'in',\n",
       " 'its',\n",
       " 'early',\n",
       " 'days',\n",
       " 'If',\n",
       " 'the',\n",
       " 'first',\n",
       " 'vaccines',\n",
       " 'made',\n",
       " 'available',\n",
       " 'only',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'minimum',\n",
       " 'they',\n",
       " 'may',\n",
       " 'be',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'others',\n",
       " 'that',\n",
       " 'prove',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'more',\n",
       " 'people',\n",
       " 'But',\n",
       " 'with',\n",
       " 'more',\n",
       " 'than',\n",
       " '1',\n",
       " 'million',\n",
       " 'deaths',\n",
       " 'from',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'worldwide',\n",
       " 'and',\n",
       " 'U',\n",
       " 'S',\n",
       " 'deaths',\n",
       " 'surpassing',\n",
       " '200',\n",
       " '000',\n",
       " 'the',\n",
       " 'urgency',\n",
       " 'in',\n",
       " 'finding',\n",
       " 'a',\n",
       " 'vaccine',\n",
       " 'that',\n",
       " 'safely',\n",
       " 'helps',\n",
       " 'at',\n",
       " 'least',\n",
       " 'some',\n",
       " 'people',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'forefront']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print(len(tokens)) # size of the vocabulary                  \n",
    "print(tokens)                     \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: COVID-19, doesn't, couldn't, 200,000 are split into two words\n",
    "\n",
    "re.findall(r\"\\w+\", text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's word tokenizer does the following steps:\n",
    "* split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
    "* treat most punctuation characters as separate tokens\n",
    "* split off commas and single quotes, when followed by whitespace\n",
    "* separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "['The', 'FDA', \"'s\", 'setting', 'a', 'minimum', 'recommendation', 'for', 'efficacy', 'does', \"n't\", 'mean', 'vaccines', 'could', \"n't\", 'perform', 'better', '.', 'The', 'benchmark', 'is', 'also', 'a', 'reminder', 'that', 'COVID-19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', '.', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', ',', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', '.', 'But', 'with', 'more', 'than', '1', 'million', 'deaths', 'from', 'COVID-19', 'worldwide', '—', 'and', 'U.S.', 'deaths', 'surpassing', '200,000', '—', 'the', 'urgency', 'in', 'finding', 'a', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront', '.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print(len(tokens) )                   \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. COVID-19, 200,000 are not split by punctuations\n",
    "# doesn't becomes does n't\n",
    "# cons: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'FDA',\n",
       " 's',\n",
       " 'setting',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'recommendation',\n",
       " 'for',\n",
       " 'efficacy',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'mean',\n",
       " 'vaccines',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'perform',\n",
       " 'better',\n",
       " '',\n",
       " 'The',\n",
       " 'benchmark',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'reminder',\n",
       " 'that',\n",
       " 'COVID-19',\n",
       " 'vaccine',\n",
       " 'development',\n",
       " 'is',\n",
       " 'in',\n",
       " 'its',\n",
       " 'early',\n",
       " 'days',\n",
       " '',\n",
       " 'If',\n",
       " 'the',\n",
       " 'first',\n",
       " 'vaccines',\n",
       " 'made',\n",
       " 'available',\n",
       " 'only',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'minimum',\n",
       " '',\n",
       " 'they',\n",
       " 'may',\n",
       " 'be',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'others',\n",
       " 'that',\n",
       " 'prove',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'more',\n",
       " 'people',\n",
       " '',\n",
       " 'But',\n",
       " 'with',\n",
       " 'more',\n",
       " 'than',\n",
       " '1',\n",
       " 'million',\n",
       " 'deaths',\n",
       " 'from',\n",
       " 'COVID-19',\n",
       " 'worldwide',\n",
       " '',\n",
       " 'and',\n",
       " 'U.S',\n",
       " 'deaths',\n",
       " 'surpassing',\n",
       " '200,000',\n",
       " '',\n",
       " 'the',\n",
       " 'urgency',\n",
       " 'in',\n",
       " 'finding',\n",
       " 'a',\n",
       " 'vaccine',\n",
       " 'that',\n",
       " 'safely',\n",
       " 'helps',\n",
       " 'at',\n",
       " 'least',\n",
       " 'some',\n",
       " 'people',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'forefront',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "['The', 'FDA', 's', 'setting', 'a', 'minimum', 'recommendation', 'for', 'efficacy', 'does', \"n't\", 'mean', 'vaccines', 'could', \"n't\", 'perform', 'better', 'The', 'benchmark', 'is', 'also', 'a', 'reminder', 'that', 'COVID-19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', 'But', 'with', 'more', 'than', '1', 'million', 'deaths', 'from', 'COVID-19', 'worldwide', 'and', 'U.S', 'deaths', 'surpassing', '200,000', 'the', 'urgency', 'in', 'finding', 'a', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.3 remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "\n",
    "tokens=[token.strip(string.punctuation+'—') for token in tokens]\n",
    "tokens\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens \\\n",
    "        if token.strip()!='']\n",
    "print(len(tokens) )\n",
    "print(tokens)  \n",
    "\n",
    "# Note '—' is still kept since it's not in the punctuation list. How to remove it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's regular expression tokinizer (customizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "['The', \"FDA's\", 'setting', 'minimum', 'recommendation', 'for', 'efficacy', \"doesn't\", 'mean', 'vaccines', \"couldn't\", 'perform', 'better', 'The', 'benchmark', 'is', 'also', 'reminder', 'that', 'COVID-19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', 'But', 'with', 'more', 'than', 'million', 'deaths', 'from', 'COVID-19', 'worldwide', 'and', 'deaths', 'surpassing', '200,000', 'the', 'urgency', 'in', 'finding', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.4 NLTK's regular expression tokenizer \n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as:\n",
    "# (1) must start with a word character  \\w\n",
    "# (2) then contain zero or more word characters,\"-\", \n",
    "#     or \"'\" in the middle [\\w\\'-]*\n",
    "# (3) must end with a word character \\w\n",
    "# e.g. COVID-19,  should not be split\n",
    "\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The FDA's setting a minimum recommendation for efficacy doesn't mean vaccines couldn't perform better.\",\n",
       " 'The benchmark is also a reminder that COVID-19 vaccine development is in its early days.',\n",
       " 'If the first vaccines made available only meet the minimum, they may be replaced by others that prove to protect more people.',\n",
       " 'But with more than 1 million deaths from COVID-19 worldwide — and U.S. deaths surpassing 200,000 — the urgency in finding a vaccine that safely helps at least some people is at the forefront.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences\n",
    "\n",
    "# what patterns can be used to segment text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Phrases: Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', \"FDA's\"), (\"FDA's\", 'setting'), ('setting', 'minimum'), ('minimum', 'recommendation'), ('recommendation', 'for'), ('for', 'efficacy'), ('efficacy', \"doesn't\"), (\"doesn't\", 'mean'), ('mean', 'vaccines'), ('vaccines', \"couldn't\"), (\"couldn't\", 'perform'), ('perform', 'better'), ('better', 'The'), ('The', 'benchmark'), ('benchmark', 'is'), ('is', 'also'), ('also', 'reminder'), ('reminder', 'that'), ('that', 'COVID-19'), ('COVID-19', 'vaccine'), ('vaccine', 'development'), ('development', 'is'), ('is', 'in'), ('in', 'its'), ('its', 'early'), ('early', 'days'), ('days', 'If'), ('If', 'the'), ('the', 'first'), ('first', 'vaccines'), ('vaccines', 'made'), ('made', 'available'), ('available', 'only'), ('only', 'meet'), ('meet', 'the'), ('the', 'minimum'), ('minimum', 'they'), ('they', 'may'), ('may', 'be'), ('be', 'replaced'), ('replaced', 'by'), ('by', 'others'), ('others', 'that'), ('that', 'prove'), ('prove', 'to'), ('to', 'protect'), ('protect', 'more'), ('more', 'people'), ('people', 'But'), ('But', 'with'), ('with', 'more'), ('more', 'than'), ('than', 'million'), ('million', 'deaths'), ('deaths', 'from'), ('from', 'COVID-19'), ('COVID-19', 'worldwide'), ('worldwide', 'and'), ('and', 'deaths'), ('deaths', 'surpassing'), ('surpassing', '200,000'), ('200,000', 'the'), ('the', 'urgency'), ('urgency', 'in'), ('in', 'finding'), ('finding', 'vaccine'), ('vaccine', 'that'), ('that', 'safely'), ('safely', 'helps'), ('helps', 'at'), ('at', 'least'), ('least', 'some'), ('some', 'people'), ('people', 'is'), ('is', 'at'), ('at', 'the'), ('the', 'forefront')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', \"FDA's\", 'setting'),\n",
       " (\"FDA's\", 'setting', 'minimum'),\n",
       " ('setting', 'minimum', 'recommendation'),\n",
       " ('minimum', 'recommendation', 'for'),\n",
       " ('recommendation', 'for', 'efficacy'),\n",
       " ('for', 'efficacy', \"doesn't\"),\n",
       " ('efficacy', \"doesn't\", 'mean'),\n",
       " (\"doesn't\", 'mean', 'vaccines'),\n",
       " ('mean', 'vaccines', \"couldn't\"),\n",
       " ('vaccines', \"couldn't\", 'perform'),\n",
       " (\"couldn't\", 'perform', 'better'),\n",
       " ('perform', 'better', 'The'),\n",
       " ('better', 'The', 'benchmark'),\n",
       " ('The', 'benchmark', 'is'),\n",
       " ('benchmark', 'is', 'also'),\n",
       " ('is', 'also', 'reminder'),\n",
       " ('also', 'reminder', 'that'),\n",
       " ('reminder', 'that', 'COVID-19'),\n",
       " ('that', 'COVID-19', 'vaccine'),\n",
       " ('COVID-19', 'vaccine', 'development'),\n",
       " ('vaccine', 'development', 'is'),\n",
       " ('development', 'is', 'in'),\n",
       " ('is', 'in', 'its'),\n",
       " ('in', 'its', 'early'),\n",
       " ('its', 'early', 'days'),\n",
       " ('early', 'days', 'If'),\n",
       " ('days', 'If', 'the'),\n",
       " ('If', 'the', 'first'),\n",
       " ('the', 'first', 'vaccines'),\n",
       " ('first', 'vaccines', 'made'),\n",
       " ('vaccines', 'made', 'available'),\n",
       " ('made', 'available', 'only'),\n",
       " ('available', 'only', 'meet'),\n",
       " ('only', 'meet', 'the'),\n",
       " ('meet', 'the', 'minimum'),\n",
       " ('the', 'minimum', 'they'),\n",
       " ('minimum', 'they', 'may'),\n",
       " ('they', 'may', 'be'),\n",
       " ('may', 'be', 'replaced'),\n",
       " ('be', 'replaced', 'by'),\n",
       " ('replaced', 'by', 'others'),\n",
       " ('by', 'others', 'that'),\n",
       " ('others', 'that', 'prove'),\n",
       " ('that', 'prove', 'to'),\n",
       " ('prove', 'to', 'protect'),\n",
       " ('to', 'protect', 'more'),\n",
       " ('protect', 'more', 'people'),\n",
       " ('more', 'people', 'But'),\n",
       " ('people', 'But', 'with'),\n",
       " ('But', 'with', 'more'),\n",
       " ('with', 'more', 'than'),\n",
       " ('more', 'than', 'million'),\n",
       " ('than', 'million', 'deaths'),\n",
       " ('million', 'deaths', 'from'),\n",
       " ('deaths', 'from', 'COVID-19'),\n",
       " ('from', 'COVID-19', 'worldwide'),\n",
       " ('COVID-19', 'worldwide', 'and'),\n",
       " ('worldwide', 'and', 'deaths'),\n",
       " ('and', 'deaths', 'surpassing'),\n",
       " ('deaths', 'surpassing', '200,000'),\n",
       " ('surpassing', '200,000', 'the'),\n",
       " ('200,000', 'the', 'urgency'),\n",
       " ('the', 'urgency', 'in'),\n",
       " ('urgency', 'in', 'finding'),\n",
       " ('in', 'finding', 'vaccine'),\n",
       " ('finding', 'vaccine', 'that'),\n",
       " ('vaccine', 'that', 'safely'),\n",
       " ('that', 'safely', 'helps'),\n",
       " ('safely', 'helps', 'at'),\n",
       " ('helps', 'at', 'least'),\n",
       " ('at', 'least', 'some'),\n",
       " ('least', 'some', 'people'),\n",
       " ('some', 'people', 'is'),\n",
       " ('people', 'is', 'at'),\n",
       " ('is', 'at', 'the'),\n",
       " ('at', 'the', 'forefront')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams from last step\n",
    "# nltk.bigram returns an iterator\n",
    "\n",
    "bigrams=list(nltk.bigrams(tokens))  # tokens are created in Exercise 3.1.4\n",
    "print(bigrams)\n",
    "\n",
    "# trigrams\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: an expression consisting of two or more words that correspond to some conventional way of saying things, e.g. red wine, United States, balance sheet etc.\n",
    "    - Collocations are not fully compositional in that there is usually an element of meaning added to the combination.\n",
    " - Question: how to find collocations?\n",
    "    - Suppose you have a rich collection of text, e.g. english-web.txt\n",
    "    - How to find good collocations from this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'),\n",
       " (',', '\"'),\n",
       " ('of', 'the'),\n",
       " (\"'\", 's'),\n",
       " ('in', 'the'),\n",
       " ('said', ','),\n",
       " ('said', 'to'),\n",
       " ('.', 'He'),\n",
       " ('the', 'land'),\n",
       " ('.', 'The'),\n",
       " (',', 'the'),\n",
       " (\"'\", 't'),\n",
       " ('him', ','),\n",
       " ('I', 'will'),\n",
       " ('to', 'the'),\n",
       " (',', 'that'),\n",
       " ('and', 'the'),\n",
       " ('to', 'him'),\n",
       " ('land', 'of'),\n",
       " ('the', 'earth')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1.\n",
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "# bigram association measures\n",
    "# different measures, e.g. frequency, are implemented\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# First load text from the file and create unigram tokens\n",
    "# Then create bigrams from the tokens\n",
    "words=nltk.corpus.genesis.words('english-web.txt')\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find the top 10 bigrams by frequency\n",
    "finder.nbest(bigram_measures.raw_freq, 20) \n",
    "\n",
    "# Note that the most frequent bigrams are very odd\n",
    "# how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('God', 'said'),\n",
       " ('one', 'hundred'),\n",
       " ('Jacob', 'said'),\n",
       " ('Yahweh', 'God'),\n",
       " ('Yahweh', 'said'),\n",
       " ('years', 'old'),\n",
       " ('seven', 'years'),\n",
       " ('Joseph', 'said'),\n",
       " ('every', 'man'),\n",
       " ('five', 'years')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words)\n",
    "# Note: negation words\n",
    "\n",
    "# apply two conditions to each word of a bigram\n",
    "# if any word satisfies one of the conditions, the entire phrase will be filtered out\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "# or w in string.punctuation\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# better?\n",
    "# most of them are in the pattern of \"xxx said\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 How to find collocations - PMI\n",
    "- By **frequency** (perhaps with filter)\n",
    "- **Pointwise Mutual Information (PMI)**\n",
    "  - giving two words $w_1, w_2$, $$PMI(w_1,w_2)=\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}$$\n",
    "  - Some observations:\n",
    "    - if $w_1$ and $w_2$ are independent, $PMI(w_1,w_2)=0$\n",
    "    - if $w_1$ is completely dependent on $w_2$, i.e. $p(w_1,w_2)=p(w_2)$, $PMI(w_1,w_2)=\\log\\frac{1}{p(w_1)}$. In this case, what if $w_1$ just appears once in the corpus? \n",
    "    - measures how much more likely the words co-occur than if they were independent\n",
    "    - PMI favors less frequent collocations \n",
    "    - how to fix it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Allon', 'Bacuth'),\n",
       " ('Ashteroth', 'Karnaim'),\n",
       " ('Ben', 'Ammi'),\n",
       " ('En', 'Mishpat'),\n",
       " ('Jegar', 'Sahadutha'),\n",
       " ('Salt', 'Sea'),\n",
       " ('Whoever', 'sheds'),\n",
       " ('appoint', 'overseers'),\n",
       " ('aromatic', 'resin'),\n",
       " ('cutting', 'instrument')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1.1 Metrics for Collocations\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find top-n bigrams by pmi\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('burnt', 'offering'),\n",
       " ('Paddan', 'Aram'),\n",
       " ('living', 'creature'),\n",
       " ('young', 'lady'),\n",
       " ('little', 'ones'),\n",
       " ('Be', 'fruitful'),\n",
       " ('still', 'alive'),\n",
       " ('savory', 'food'),\n",
       " ('creeping', 'thing'),\n",
       " ('find', 'favor')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "# fix the problem that PMI favors less frequent collocations\n",
    "\n",
    "finder.apply_freq_filter(5)  #5\n",
    "finder.nbest(bigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 How to find collocations - NPMI and others\n",
    "- **Normalized Pointwise Mutual Information (`NPMI`)**\n",
    "   - If $w_1$ and $w_2$ always occur together, i.e., $p(w_1)=p(w_2)=p(w_1,w_2)$, PMI reaches the maximum: $$PMI(w_1,w_2)=-\\log{p(w_1)}=-\\log{p(w_2)}=-\\log{p(w_1,w_2)}$$\n",
    "   - Normalized PMI is the PMI divided by the upper bound:\n",
    "   $$NPMI(w_1,w_2)=\\frac{\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}}{-\\log{p(w_1,w_2)}}$$\n",
    "- Another simple method by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf):\n",
    "\n",
    "    - $Score(w_1, w_2)=\\frac{count(w_1,w_2)-\\delta}{count(w_1)*count(w_2)}, \\text{where}~\\delta~\\text{is the minimum collocation frequency} $ \n",
    "\n",
    "    - This is equivalent to PMI with a minimum collocation threshold\n",
    "- Both methods are implemented in `gensim` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5. Vocabulary \n",
    " - Vocabulary: the set of unique tokens (unigrams/phrases)  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word, Value: count of the word\n",
    "    * **nltk.FreqDist()**: a nice function for calculating frequncy of words/phrases\n",
    "        - Get the frequency of items in the parameter list \n",
    "        - Retruns an object similar to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6, 'is': 3, 'that': 3, 'minimum': 2, 'vaccines': 2, 'covid-19': 2, 'vaccine': 2, 'in': 2, 'more': 2, 'people': 2, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 words: [('the', 6), ('is', 3), ('that', 3), ('minimum', 2), ('vaccines', 2)]\n",
      "the : 6\n",
      "fda's : 1\n",
      "setting : 1\n",
      "minimum : 2\n",
      "recommendation : 1\n",
      "for : 1\n",
      "efficacy : 1\n",
      "doesn't : 1\n",
      "mean : 1\n",
      "vaccines : 2\n",
      "couldn't : 1\n",
      "perform : 1\n",
      "better : 1\n",
      "benchmark : 1\n",
      "is : 3\n",
      "also : 1\n",
      "reminder : 1\n",
      "that : 3\n",
      "covid-19 : 2\n",
      "vaccine : 2\n",
      "development : 1\n",
      "in : 2\n",
      "its : 1\n",
      "early : 1\n",
      "days : 1\n",
      "if : 1\n",
      "first : 1\n",
      "made : 1\n",
      "available : 1\n",
      "only : 1\n",
      "meet : 1\n",
      "they : 1\n",
      "may : 1\n",
      "be : 1\n",
      "replaced : 1\n",
      "by : 1\n",
      "others : 1\n",
      "prove : 1\n",
      "to : 1\n",
      "protect : 1\n",
      "more : 2\n",
      "people : 2\n",
      "but : 1\n",
      "with : 1\n",
      "than : 1\n",
      "million : 1\n",
      "deaths : 2\n",
      "from : 1\n",
      "worldwide : 1\n",
      "and : 1\n",
      "surpassing : 1\n",
      "200,000 : 1\n",
      "urgency : 1\n",
      "finding : 1\n",
      "safely : 1\n",
      "helps : 1\n",
      "at : 2\n",
      "least : 1\n",
      "some : 1\n",
      "forefront : 1\n"
     ]
    }
   ],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# first tokenize the text\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "word_dist\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 5 words:\", word_dist.most_common(5))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.5.1 Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. We sometimes find it useful, or necessary, to customize our stop words in any given application.You can define your own stop words!\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'covid-19', 'virus']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"fda's\": 1,\n",
       " 'setting': 1,\n",
       " 'minimum': 2,\n",
       " 'recommendation': 1,\n",
       " 'efficacy': 1,\n",
       " 'mean': 1,\n",
       " 'vaccines': 2,\n",
       " 'perform': 1,\n",
       " 'better': 1,\n",
       " 'benchmark': 1,\n",
       " 'also': 1,\n",
       " 'reminder': 1,\n",
       " 'vaccine': 2,\n",
       " 'development': 1,\n",
       " 'early': 1,\n",
       " 'days': 1,\n",
       " 'first': 1,\n",
       " 'made': 1,\n",
       " 'available': 1,\n",
       " 'meet': 1,\n",
       " 'may': 1,\n",
       " 'replaced': 1,\n",
       " 'others': 1,\n",
       " 'prove': 1,\n",
       " 'protect': 1,\n",
       " 'people': 2,\n",
       " 'million': 1,\n",
       " 'deaths': 2,\n",
       " 'worldwide': 1,\n",
       " 'surpassing': 1,\n",
       " '200,000': 1,\n",
       " 'urgency': 1,\n",
       " 'finding': 1,\n",
       " 'safely': 1,\n",
       " 'helps': 1,\n",
       " 'least': 1,\n",
       " 'forefront': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('minimum', 2),\n",
       " ('vaccines', 2),\n",
       " ('vaccine', 2),\n",
       " ('people', 2),\n",
       " ('deaths', 2),\n",
       " (\"fda's\", 1),\n",
       " ('setting', 1),\n",
       " ('recommendation', 1),\n",
       " ('efficacy', 1),\n",
       " ('mean', 1),\n",
       " ('perform', 1),\n",
       " ('better', 1),\n",
       " ('benchmark', 1),\n",
       " ('also', 1),\n",
       " ('reminder', 1),\n",
       " ('development', 1),\n",
       " ('early', 1),\n",
       " ('days', 1),\n",
       " ('first', 1),\n",
       " ('made', 1),\n",
       " ('available', 1),\n",
       " ('meet', 1),\n",
       " ('may', 1),\n",
       " ('replaced', 1),\n",
       " ('others', 1),\n",
       " ('prove', 1),\n",
       " ('protect', 1),\n",
       " ('million', 1),\n",
       " ('worldwide', 1),\n",
       " ('surpassing', 1),\n",
       " ('200,000', 1),\n",
       " ('urgency', 1),\n",
       " ('finding', 1),\n",
       " ('safely', 1),\n",
       " ('helps', 1),\n",
       " ('least', 1),\n",
       " ('forefront', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.5.1.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"covid-19\", \"virus\"]\n",
    "print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "\n",
    "\n",
    "filtered_dict\n",
    "\n",
    "# how to sort the dictionary by value?\n",
    "sorted(filtered_dict.items(),key=lambda item: item[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 positive/negative words: sentiment analysis\n",
    "- Sentiment analysis often relies on **lists of words and phrases with positive and negative connotations**. \n",
    "- Many dictionaries of positive and negative opinion words were already developed:\n",
    "\n",
    "  - **Hu and Liu's lexicon**: http://www.cs.uic.edu/~liub/FBS/\n",
    "  - **SentiWordNet**: an excellent publicly available lexicon https://github.com/aesuli/SentiWordNet\n",
    "  - **SentiWords**: contains 155,000 English words https://arxiv.org/abs/1510.09079\n",
    "  - **WordStat**: contains more than 9164 negative and 4847 positive word patterns (https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/sentiment-dictionaries/)\n",
    "  - **SenticNet**: provides polarity associated with 50,000 natural language concepts https://sentic.net\n",
    "  - **Sentiment140**:  created from 1.6 million tweets and contains a list of words and their associations with positive and negative sentiment (https://github.com/felipebravom/StaticTwitterSent/tree/master/extra/Sentiment140-Lexicon-v0.1)\n",
    "- Opinion words are <b>domain-specific</b>. (e.g. \"power\" in political domain vs. in engergy sector)\n",
    "  - For example, for financial industry, there are a number of dictionaries for opinion words:\n",
    "     * Harvard's General Inquirer (GI): http://www.mariapinto.es/ciberabstracts/Articulos/Inquirer.html\n",
    "     * Loughran and McDonald (2015):  https://sraf.nd.edu/loughranmcdonald-master-dictionary/\n",
    "- For description of these lexicons, check https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c\n",
    "- Question: **How to select the right lexicon**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5.2.1\n",
    "# Find positive words \n",
    "text = '''the problem is that the writers, james cameron and jay cocks , were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''\n",
    "\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- **Naive sentiment analysis**:\n",
    "  - Find positive/negative words\n",
    "  - If more positive words than negative, then positive\n",
    "  - Otherwise, negative\n",
    "- Note the sentence: \n",
    "  -  \"the problem is that the writers, james cameron and jay cocks , were **<font color=\"red\">too ambitious</font>**, aiming for a film with social relevance, thrills, and drama. **<font color=\"red\">not that ambitious</font>** film-making should be discouraged; just that when it fails to achieve its goals ...\"\n",
    "- How to deal with **negation**?\n",
    "- Some useful rules:\n",
    "    - Negative sentiment: \n",
    "      - negative words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - positive words preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "    - Positive sentiment (in the similar fashion):\n",
    "      - positive words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - negative terms following a negation within  $n$ (e.g. three) words in the same sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5.2.2 # check if a positive word is preceded by negation words\n",
    "# e.g. not, too, n't, no, cannot\n",
    "\n",
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "\n",
    "# what if a positive word is preceded \n",
    "# by a negation within N words? \n",
    "# e.g. 'does not make any customer happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
