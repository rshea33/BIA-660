{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (II)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf\n",
    " - https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will see other steps\n",
    "\n",
    "   * Tokenization: split documents into individual words, phrases, or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * **POS (part of speech) Tagging**  \n",
    "   * **Normalization: Stemming, Lemmatization**\n",
    "   * **Named Entity Recognition (NER)**\n",
    "   * **Term Frequency and Inverse Dcoument Frequency (TF-IDF)**\n",
    "   * **Create document-to-term matrix (bag of words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oil prices soar to all-time record. stocks end up near year end. money funds rose in latest week. stocks up; traders eye crude oil prices. dollar rising broadly on record trade gain'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample text for analysis\n",
    "\n",
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text=\". \".join(news).lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS (Part of Speech) Tagging\n",
    "\n",
    " - What is POS Tagging:\n",
    "   * The process of marking up a word in a text as corresponding to a particular part of speech (e.g. nouns, verbs, adjectives, adverbs etc.), based on both **its definition**, as well as its **context** — adjacent and related words in a phrase, sentence, or paragraph. \n",
    " - Why POS Tagging: \n",
    "   * **disambiguation**: A word may have different meanings. POS tag is a potential strong signal for word sense disambiguation. For example, \"I fish a fish\"\n",
    "   * **Phrase extraction**: Use POS rules to define accepted phrases (or information unit), or collocations for indexing and retrieval:\n",
    "     * Adj + Noun, e.g. nice house\n",
    "     * Verb + Noun, e.g. play football\n",
    "     * typical collocation patterns (https://nlp.stanford.edu/fsnlp/promo/colloc.pdf):\n",
    "       - Adj + Noun: e.g. linear function\n",
    "       - Noun + Noun: e.g. regression coefficient\n",
    "       - Adj + Adj + Noun: e.g. Gaussian random variable\n",
    "       - Noun + Adj + Noun: e.g. mean squared error\n",
    "       - Noun + Noun + Noun: e.g. class probability function\n",
    "       - Noun + Preposition + Noun: e.g. dregrees of freedom\n",
    "   * **Filter tokens**:  some POS have less importance in retrieval, e.g. stopwords such as ‘a’, ‘an’, ‘the’, and other glue words like 'in', 'on', 'of' etc.\n",
    "   * Find other forms of a word based on POS\n",
    "        * Noun: plural and singular\n",
    "        * Verb: past, present and future tense\n",
    "        * Adjective: positive, comparative, and superlative\n",
    " - List of Penn Treebank Tags can be found at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    " - A tagger (program for tagging) is trained based on a corpus using machine learning approaches. It may not be very accurate when applying it your corpus.\n",
    "   - Stanford tagger\n",
    "   - NLTK default tagger (PerceptronTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.1. To find all tags in treebank\n",
    "nltk.help.upenn_tagset()\n",
    "\n",
    "# find the meaning of a specific tag\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oil', 'NN'),\n",
       " ('prices', 'NNS'),\n",
       " ('soar', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('all-time', 'JJ'),\n",
       " ('record', 'NN'),\n",
       " ('.', '.'),\n",
       " ('stocks', 'NNS'),\n",
       " ('end', 'VBP'),\n",
       " ('up', 'RP'),\n",
       " ('near', 'IN'),\n",
       " ('year', 'NN'),\n",
       " ('end', 'NN'),\n",
       " ('.', '.'),\n",
       " ('money', 'NN'),\n",
       " ('funds', 'NNS'),\n",
       " ('rose', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('latest', 'JJS'),\n",
       " ('week', 'NN'),\n",
       " ('.', '.'),\n",
       " ('stocks', 'NNS'),\n",
       " ('up', 'RP'),\n",
       " (';', ':'),\n",
       " ('traders', 'NNS'),\n",
       " ('eye', 'NN'),\n",
       " ('crude', 'VBP'),\n",
       " ('oil', 'NN'),\n",
       " ('prices', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('dollar', 'NN'),\n",
       " ('rising', 'VBG'),\n",
       " ('broadly', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('record', 'NN'),\n",
       " ('trade', 'NN'),\n",
       " ('gain', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4.2. NLTK POS Tagging\n",
    "\n",
    "# The input to the tagging function is a list of words\n",
    "\n",
    "# tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# tag each tokenized word\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "\n",
    "tagged_tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('oil', 'NN'), ('prices', 'NNS')), (('prices', 'NNS'), ('soar', 'VB')), (('soar', 'VB'), ('to', 'TO')), (('to', 'TO'), ('all-time', 'JJ')), (('all-time', 'JJ'), ('record', 'NN')), (('record', 'NN'), ('.', '.')), (('.', '.'), ('stocks', 'NNS')), (('stocks', 'NNS'), ('end', 'VBP')), (('end', 'VBP'), ('up', 'RP')), (('up', 'RP'), ('near', 'IN')), (('near', 'IN'), ('year', 'NN')), (('year', 'NN'), ('end', 'NN')), (('end', 'NN'), ('.', '.')), (('.', '.'), ('money', 'NN')), (('money', 'NN'), ('funds', 'NNS')), (('funds', 'NNS'), ('rose', 'VBD')), (('rose', 'VBD'), ('in', 'IN')), (('in', 'IN'), ('latest', 'JJS')), (('latest', 'JJS'), ('week', 'NN')), (('week', 'NN'), ('.', '.')), (('.', '.'), ('stocks', 'NNS')), (('stocks', 'NNS'), ('up', 'RP')), (('up', 'RP'), (';', ':')), ((';', ':'), ('traders', 'NNS')), (('traders', 'NNS'), ('eye', 'NN')), (('eye', 'NN'), ('crude', 'VBP')), (('crude', 'VBP'), ('oil', 'NN')), (('oil', 'NN'), ('prices', 'NNS')), (('prices', 'NNS'), ('.', '.')), (('.', '.'), ('dollar', 'NN')), (('dollar', 'NN'), ('rising', 'VBG')), (('rising', 'VBG'), ('broadly', 'RB')), (('broadly', 'RB'), ('on', 'IN')), (('on', 'IN'), ('record', 'NN')), (('record', 'NN'), ('trade', 'NN')), (('trade', 'NN'), ('gain', 'NN'))]\n",
      "[('all-time', 'record'), ('latest', 'week')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.3. Extract Phrases by POS\n",
    "\n",
    "# Extract phrases in pattern of adjective + noun\n",
    "# i.e. nice house, growing market\n",
    "\n",
    "bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "print(bigrams)\n",
    "\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('JJ') \\\n",
    "         and y[1].startswith('NN')]\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prices', 'soar'), ('stocks', 'end'), ('funds', 'rose'), ('eye', 'crude'), ('dollar', 'rising')]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.4. Extract Noun+Verb, \n",
    "# i.e. prices soar\n",
    "bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "#print(bigrams)\n",
    "\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('NN') \\\n",
    "         and y[1].startswith('VB')]\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization: Stemming & Lemmatization\n",
    " - What is normalization:\n",
    "   - Converts a list of words in **different surface forms** to a more **uniform form**, e.g.\n",
    "        * a word with different forms, e.g. organize, organizes, organized, and organizing\n",
    "        * families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n",
    " - Why normalization\n",
    "   - **improve text matching**: in many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "   - reduce featue space generated from text\n",
    " - Stemming and lemmatization are two common techinques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Stemming \n",
    "\n",
    "* **Stemming**: reducing inflected (or sometimes derived) words to their **stem, base or root** form. \n",
    "   * For example, **crying** -> **cri**. \n",
    "   * Stemming may not generate a real word, but a root form. \n",
    "   * The stemming program is called stemmer. \n",
    "       * Famous stemers are Porter stemmer, Lancaster Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem of organizing/organized/organizes/organization\n",
      "organ\n",
      "organ\n",
      "organ\n",
      "organ\n",
      "\n",
      "Stem of crying\n",
      "cri\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.1.1. Stermming Using Porter Stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Stem of organizing/organized/organizes/organization\")\n",
    "print(porter_stemmer.stem('organizing'))\n",
    "print(porter_stemmer.stem('organized'))\n",
    "print(porter_stemmer.stem('organizes'))\n",
    "print(porter_stemmer.stem('organization'))\n",
    "\n",
    "\n",
    "print(\"\\nStem of crying\")\n",
    "print(porter_stemmer.stem('crying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'gener'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'gener'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'generic'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'generous'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# comparison of stemminng adverbs\n",
    "porter_stemmer.stem('fairly')\n",
    "snowball_stemmer.stem('fairly') \n",
    "\n",
    "# prevents overstemming in some cases\n",
    "porter_stemmer.stem('generically') \n",
    "porter_stemmer.stem('generous') \n",
    "\n",
    "snowball_stemmer.stem('generically') \n",
    "snowball_stemmer.stem('generous') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salti'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sal'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sale'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'sal'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lanc_stemmer = LancasterStemmer()\n",
    "\n",
    "# tends to overstem\n",
    "snowball_stemmer.stem('salty') \n",
    "lanc_stemmer.stem('salty') \n",
    "\n",
    "snowball_stemmer.stem('sales') \n",
    "lanc_stemmer.stem('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Lemmatization\n",
    "\n",
    "* **Lemmatization**: determining the lemma for a given word, \n",
    "   * A lemma is a word which stands at the head of a definition in a dictionary, e.g. run (lemma),  runs, ran and running (inflections) \n",
    "   * Lemmatization is a complex task involving understanding context and determining the part of speech of a word in a sentence \n",
    "      * e.g. \"organized\" (verb or adjective?)\n",
    "   * The widely used Lemmatization method is based on WordNet, a large lexical database of English.\n",
    "\n",
    "* **Difference** between stemming and lemmatization: \n",
    "   * a stemmer operates on a single word **without knowledge of the context**, and therefore cannot discriminate between words which have different meanings depending on part of speech. While, lemmatization **requires context and POS tags**. \n",
    "   * Stemming may not generate a real word, but lemmization always generates real words.\n",
    "   *  However, stemmers are typically easier to implement and run faster with reduced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organizing (verb) -> organize\n",
      "organized (verb) -> organize\n",
      "organized (adjective) -> organized\n",
      "organization (noun) -> organization\n",
      "crying (adjective) -> crying\n",
      "crying (verb) -> cry\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.2.1. Lemmatization\n",
    "\n",
    "# wordnet lemmatizer takes POS tag as a parameter\n",
    "# However, wordnet has its own tag set, \n",
    "# different from treebank tag set\n",
    "# The default POS tag is noun \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"organizing (verb) ->\", \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organizing', wordnet.VERB))\n",
    "print('organized (verb) ->', \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organized', wordnet.VERB))\n",
    "print('organized (adjective) ->',\\\n",
    "      wordnet_lemmatizer.lemmatize('organized', \\\n",
    "                                   wordnet.ADJ))\n",
    "print('organization (noun) ->',\\\n",
    "      wordnet_lemmatizer.lemmatize('organization'))\n",
    "print('crying (adjective) ->',\\\n",
    "      wordnet_lemmatizer.lemmatize('crying', \\\n",
    "                                   wordnet.ADJ))\n",
    "print('crying (verb) ->', \\\n",
    "      wordnet_lemmatizer.lemmatize('crying', \\\n",
    "                                   wordnet.VERB))\n",
    "\n",
    "# compare the result with Exercise 5.1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "- Definition: find and classify real word entities (Person, Organization, Event etc.) in text\n",
    "- Example: sentence \"Jim bought 300 shares of Acme Corp. in 2006\" can be annotated as \"**[Jim]<sub>Person</sub>** bought 300 shares of **[Acme Corp.]<sub>Organization</sub>** in 2006\"\n",
    "- Uses of NER:\n",
    "   *  Information Extraction: extract clear, factual information, i.e., Who did what to whom when?\n",
    "   *  Named entities can be indexed, and their relations can be extracted as a knowledge graph.\n",
    "   *  Sentiment can be attributed to companies or products. \n",
    "   *  For question answering, answers are often named entities.\n",
    "- Techniques for NER\n",
    "   * Regular expression: Telephone numbers, emails, Capital names (e.g. Capitalized word + {city,  center, river}\n",
    "      * Adantages: simple and sometimes effective\n",
    "      * Disadvantage: \n",
    "         * first word of a sentence is capitalized; sometimes, titles are all capitalized; new proper names constantly emerges (e.g. movie titles, books, etc.)\n",
    "         * proper names may be ambiguous, e.g. Jordan can be *person* or *location*\n",
    "   * Supervised learning (IOB) (https://arxiv.org/abs/cmp-lg/9505040)\n",
    "       1. Collect a set of representative training documents\n",
    "       2. Label each token for its entity class (I: inside entity, B: begining entity) or other (O)\n",
    "       3. Design feature extractors appropriate to the text and classes, e.g. current word, pre/next word, pos tags etc.\n",
    "       4. Train a sequence classifier to predict the labels from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     S                                                      \n",
      "     ________________________________|_____________________________________                  \n",
      "    |        |        |        |     |      |     |   PERSON          ORGANIZATION          \n",
      "    |        |        |        |     |      |     |     |        __________|___________      \n",
      "bought/VBD 300/CD shares/NNS of/IN in/IN 2006/CD ./. Jim/NNP Acme/NNP              Corp./NNP\n",
      "\n",
      "PERSON [[('Jim', 'NNP')]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6.1. Use NLTK for Named Entity Recognition\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, Tree\n",
    "\n",
    "\n",
    "sentence = \"Jim bought 300 shares of Acme Corp. in 2006.\"\n",
    "\n",
    "\n",
    "# the input to ne_chunk is list of (token, pos tag) tuples\n",
    "ner_tree=ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# ne_chunk returns a tree\n",
    "# print the tree\n",
    "Tree.fromstring(str(ner_tree)).pretty_print()\n",
    "\n",
    "\n",
    "# get PERSON out of the tree\n",
    "person=[]\n",
    "for t in ner_tree.subtrees():\n",
    "    if t.label() == 'PERSON':\n",
    "        person.append(t.leaves())\n",
    "print(\"PERSON\",person)\n",
    "\n",
    "\n",
    "# how to extract organization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jim', 'NNP', 'B-PERSON'), ('bought', 'VBD', 'O'), ('300', 'CD', 'O'), ('shares', 'NNS', 'O'), ('of', 'IN', 'O'), ('Acme', 'NNP', 'B-ORGANIZATION'), ('Corp.', 'NNP', 'I-ORGANIZATION'), ('in', 'IN', 'O'), ('2006', 'CD', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import conlltags2tree, tree2conlltags\n",
    "\n",
    "iob_tags = tree2conlltags(ner_tree)\n",
    "print(iob_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    " - Motivation: How to identify important words (or phrases, named entities) in a text in a collecton or corpus? When search for documents, we'd like to have these important words are matched.\n",
    " - Intuition: \n",
    "   * In a document, if a word/term/phrase is repeated many times, it is likely to be important. \n",
    "   * However, if it appears in most of the documents in the corpus, then it has little discriminating power in determining relevance. \n",
    "   * For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. Search by \"auto\" you may get all the documents. \n",
    " - **TF-IDF**: is composed by two terms: \n",
    "      - `TF (Term Frequency)`: which measures how frequently a term, say w, occurs in a document. \n",
    "      - `IDF (Inverse Document Frequency)`: measures how important a term is within the corpus. \n",
    " \n",
    " - TF-IDF provides another way to remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Term Frequency (TF)\n",
    "- Measures how frequently a term, say w, occurs in a document, say $d$. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. \n",
    "- Thus, the frequency of $w$ in $d$, denoted as $freq(w,d)$ is often divided by the document length (a.k.a. the total number of terms in the document, denoted as $|d|$) as a way of normalization: $$tf(w,d) = \\frac{freq(w,d)}{|d|}$$\n",
    "- Example: d=\"Stocks end up near year end\"\n",
    "   * `tf('Stocks',d)=?`\n",
    "   * `tf('end',d)=?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Inverse Document Frequency (IDF)\n",
    "- Measures how important a term is within the corpus. \n",
    "- However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. \n",
    "- Thus we need to weigh down the frequent terms while scale up the rare ones. \n",
    "- Let $|D|$ denote the number of documents, $df(w,D)$ denotes the number of documents with term $w$ in them. Then, $$idf(w) = ln(\\frac{|D|}{df(w,D)})+1$$ Or a smoothed version: $$idf(w) = ln(\\frac{|D|+1}{df(w,D)+1})+1$$\n",
    "- Examples: \n",
    "  * Considering dataset:\n",
    "       1. \"Oil prices soar to all-time record\", \n",
    "       2. \"Stocks end up near year end\", \n",
    "       3. \"Money funds rose in latest week\",\n",
    "       4. \"Stocks up; traders eye crude oil prices\",\n",
    "       5. \"Dollar rising broadly on record trade gain\"\n",
    "  * `idf('Stocks')=`?\n",
    "  * `idf('all-time')=`?\n",
    "  * Discussion:\n",
    "     * What words get very low IDF score?\n",
    "     * What words get very high IDF score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. TF-IDF \n",
    "- Let $s(w,d)=tf(w,d) * idf(w)$, normalize the TF-IDF score of each word in a document normalized by the Euclidean norm, then \n",
    "   $$tfidf(w,d)=\\frac{s(w,d)}{\\sqrt{\\sum_{w \\in d}{s(w,d)^2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.1. computing tf-idf\n",
    "\n",
    "\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# numpy is the package for matrix caculation\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "docs=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: FreqDist({'oil': 1, 'prices': 1, 'soar': 1, 'all-time': 1, 'record': 1}),\n",
       " 1: FreqDist({'end': 2, 'stocks': 1, 'near': 1, 'year': 1}),\n",
       " 2: FreqDist({'money': 1, 'funds': 1, 'rose': 1, 'latest': 1, 'week': 1}),\n",
       " 3: FreqDist({'stocks': 1, 'traders': 1, 'eye': 1, 'crude': 1, 'oil': 1, 'prices': 1}),\n",
       " 4: FreqDist({'dollar': 1, 'rising': 1, 'broadly': 1, 'record': 1, 'trade': 1, 'gain': 1})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1. get tokens of each document as list\n",
    "\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, or lemmatization here\n",
    "    \n",
    "    # create token count dictionary\n",
    "    token_count=nltk.FreqDist(tokens)\n",
    "    \n",
    "    # or you can create dictionary by yourself\n",
    "    #token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "# step 2. process all documents to \n",
    "# a dictionary of dictionaries\n",
    "docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil</th>\n",
       "      <th>prices</th>\n",
       "      <th>soar</th>\n",
       "      <th>all-time</th>\n",
       "      <th>record</th>\n",
       "      <th>stocks</th>\n",
       "      <th>end</th>\n",
       "      <th>near</th>\n",
       "      <th>year</th>\n",
       "      <th>money</th>\n",
       "      <th>...</th>\n",
       "      <th>latest</th>\n",
       "      <th>week</th>\n",
       "      <th>traders</th>\n",
       "      <th>eye</th>\n",
       "      <th>crude</th>\n",
       "      <th>dollar</th>\n",
       "      <th>rising</th>\n",
       "      <th>broadly</th>\n",
       "      <th>trade</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   oil  prices  soar  all-time  record  stocks  end  near  year  money  ...  \\\n",
       "0  1.0     1.0   1.0       1.0     1.0     NaN  NaN   NaN   NaN    NaN  ...   \n",
       "3  1.0     1.0   NaN       NaN     NaN     1.0  NaN   NaN   NaN    NaN  ...   \n",
       "4  NaN     NaN   NaN       NaN     1.0     NaN  NaN   NaN   NaN    NaN  ...   \n",
       "1  NaN     NaN   NaN       NaN     NaN     1.0  2.0   1.0   1.0    NaN  ...   \n",
       "2  NaN     NaN   NaN       NaN     NaN     NaN  NaN   NaN   NaN    1.0  ...   \n",
       "\n",
       "   latest  week  traders  eye  crude  dollar  rising  broadly  trade  gain  \n",
       "0     NaN   NaN      NaN  NaN    NaN     NaN     NaN      NaN    NaN   NaN  \n",
       "3     NaN   NaN      1.0  1.0    1.0     NaN     NaN      NaN    NaN   NaN  \n",
       "4     NaN   NaN      NaN  NaN    NaN     1.0     1.0      1.0    1.0   1.0  \n",
       "1     NaN   NaN      NaN  NaN    NaN     NaN     NaN      NaN    NaN   NaN  \n",
       "2     1.0   1.0      NaN  NaN    NaN     NaN     NaN      NaN    NaN   NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil</th>\n",
       "      <th>prices</th>\n",
       "      <th>soar</th>\n",
       "      <th>all-time</th>\n",
       "      <th>record</th>\n",
       "      <th>stocks</th>\n",
       "      <th>end</th>\n",
       "      <th>near</th>\n",
       "      <th>year</th>\n",
       "      <th>money</th>\n",
       "      <th>...</th>\n",
       "      <th>latest</th>\n",
       "      <th>week</th>\n",
       "      <th>traders</th>\n",
       "      <th>eye</th>\n",
       "      <th>crude</th>\n",
       "      <th>dollar</th>\n",
       "      <th>rising</th>\n",
       "      <th>broadly</th>\n",
       "      <th>trade</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   oil  prices  soar  all-time  record  stocks  end  near  year  money  ...  \\\n",
       "0  1.0     1.0   1.0       1.0     1.0     0.0  0.0   0.0   0.0    0.0  ...   \n",
       "3  1.0     1.0   0.0       0.0     0.0     1.0  0.0   0.0   0.0    0.0  ...   \n",
       "4  0.0     0.0   0.0       0.0     1.0     0.0  0.0   0.0   0.0    0.0  ...   \n",
       "1  0.0     0.0   0.0       0.0     0.0     1.0  2.0   1.0   1.0    0.0  ...   \n",
       "2  0.0     0.0   0.0       0.0     0.0     0.0  0.0   0.0   0.0    1.0  ...   \n",
       "\n",
       "   latest  week  traders  eye  crude  dollar  rising  broadly  trade  gain  \n",
       "0     0.0   0.0      0.0  0.0    0.0     0.0     0.0      0.0    0.0   0.0  \n",
       "3     0.0   0.0      1.0  1.0    1.0     0.0     0.0      0.0    0.0   0.0  \n",
       "4     0.0   0.0      0.0  0.0    0.0     1.0     1.0      1.0    1.0   1.0  \n",
       "1     0.0   0.0      0.0  0.0    0.0     0.0     0.0      0.0    0.0   0.0  \n",
       "2     1.0   1.0      0.0  0.0    0.0     0.0     0.0      0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil</th>\n",
       "      <th>prices</th>\n",
       "      <th>soar</th>\n",
       "      <th>all-time</th>\n",
       "      <th>record</th>\n",
       "      <th>stocks</th>\n",
       "      <th>end</th>\n",
       "      <th>near</th>\n",
       "      <th>year</th>\n",
       "      <th>money</th>\n",
       "      <th>...</th>\n",
       "      <th>latest</th>\n",
       "      <th>week</th>\n",
       "      <th>traders</th>\n",
       "      <th>eye</th>\n",
       "      <th>crude</th>\n",
       "      <th>dollar</th>\n",
       "      <th>rising</th>\n",
       "      <th>broadly</th>\n",
       "      <th>trade</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   oil  prices  soar  all-time  record  stocks  end  near  year  money  ...  \\\n",
       "0  1.0     1.0   1.0       1.0     1.0     0.0  0.0   0.0   0.0    0.0  ...   \n",
       "1  0.0     0.0   0.0       0.0     0.0     1.0  2.0   1.0   1.0    0.0  ...   \n",
       "2  0.0     0.0   0.0       0.0     0.0     0.0  0.0   0.0   0.0    1.0  ...   \n",
       "3  1.0     1.0   0.0       0.0     0.0     1.0  0.0   0.0   0.0    0.0  ...   \n",
       "4  0.0     0.0   0.0       0.0     1.0     0.0  0.0   0.0   0.0    0.0  ...   \n",
       "\n",
       "   latest  week  traders  eye  crude  dollar  rising  broadly  trade  gain  \n",
       "0     0.0   0.0      0.0  0.0    0.0     0.0     0.0      0.0    0.0   0.0  \n",
       "1     0.0   0.0      0.0  0.0    0.0     0.0     0.0      0.0    0.0   0.0  \n",
       "2     1.0   1.0      0.0  0.0    0.0     0.0     0.0      0.0    0.0   0.0  \n",
       "3     0.0   0.0      1.0  1.0    1.0     0.0     0.0      0.0    0.0   0.0  \n",
       "4     0.0   0.0      0.0  0.0    0.0     1.0     1.0      1.0    1.0   1.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3. get document-term matrix\n",
    "# contruct a document-term matrix where \n",
    "# each row is a doc \n",
    "# each column is a token\n",
    "# and the value is the frequency of the token\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# since we have a small corpus, we can use dataframe \n",
    "# to get document-term matrix\n",
    "# but don't use this when you have a large corpus\n",
    "\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, \\\n",
    "                           orient=\"index\" \n",
    "                          )\n",
    "dtm\n",
    "\n",
    "dtm=dtm.fillna(0)\n",
    "dtm\n",
    "\n",
    "# sort by index (i.e. doc id)\n",
    "dtm = dtm.sort_index(axis = 0)\n",
    "dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., 6., 6.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.4 , 0.2 , 0.2 , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.2 ,\n",
       "        0.2 , 0.2 , 0.2 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.17, 0.17, 0.  , 0.  , 0.  , 0.17, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.17, 0.17, 0.17, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.17, 0.17, 0.17, 0.17]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5, 22)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 4. get normalized term frequency (tf) matrix\n",
    "\n",
    "# convert dtm to numpy arrays\n",
    "dtm2=dtm.values\n",
    "\n",
    "# sum the value of each row\n",
    "doc_len=dtm2.sum(axis=1)\n",
    "doc_len\n",
    "\n",
    "# divide dtm matrix by the doc length matrix\n",
    "# note broadcasting \n",
    "tf=np.divide(dtm2, doc_len[:,None])  \n",
    "\n",
    "# set float precision to print nicely\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "tf\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.92, 1.92, 2.61, 2.61, 1.92, 1.92, 2.61, 2.61, 2.61, 2.61, 2.61,\n",
       "       2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(22,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smoothed IDF Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.69, 1.69, 2.1 , 2.1 , 1.69, 1.69, 2.1 , 2.1 , 2.1 , 2.1 , 2.1 ,\n",
       "       2.1 , 2.1 , 2.1 , 2.1 , 2.1 , 2.1 , 2.1 , 2.1 , 2.1 , 2.1 , 2.1 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 5. get idf\n",
    "\n",
    "# get document frequency\n",
    "df=np.where(dtm2>0,1,0)\n",
    "df\n",
    "\n",
    "# get idf\n",
    "idf=np.log(np.divide(len(docs), \\\n",
    "        np.sum(df, axis=0)))+1\n",
    "print(\"\\nIDF Matrix\")\n",
    "idf\n",
    "idf.shape\n",
    "\n",
    "# what is the size of idf array?\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "print(\"\\nSmoothed IDF Matrix\")\n",
    "smoothed_idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.38, 0.38, 0.52, 0.52, 0.38, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.38, 1.04, 0.52, 0.52, 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.52, 0.52,\n",
       "        0.52, 0.52, 0.52, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.32, 0.32, 0.  , 0.  , 0.  , 0.32, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.43, 0.43, 0.43, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.43, 0.43, 0.43, 0.43, 0.43]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.39, 0.39, 0.53, 0.53, 0.39, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.78, 0.39, 0.39, 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45, 0.45,\n",
       "        0.45, 0.45, 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.34, 0.34, 0.  , 0.  , 0.  , 0.34, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.47, 0.47, 0.47, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.42, 0.42, 0.42, 0.42, 0.42]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smoothed TF-IDF Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.41, 0.41, 0.5 , 0.5 , 0.41, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.78, 0.39, 0.39, 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45, 0.45,\n",
       "        0.45, 0.45, 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.36, 0.36, 0.  , 0.  , 0.  , 0.36, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.45, 0.45, 0.45, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.34, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.42, 0.42, 0.42, 0.42, 0.42]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 6. get tf-idf\n",
    "print(\"TF-IDF Matrix\")\n",
    "s=tf*idf\n",
    "s  # raw score\n",
    "\n",
    "# by default normalize by row\n",
    "tf_idf=normalize(tf*idf)   # is broadcast possible here?\n",
    "tf_idf\n",
    "\n",
    "print(\"\\nSmoothed TF-IDF Matrix\")\n",
    "smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "smoothed_tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF matrix gives **weight** of each word in each document\n",
    "- Documents:\n",
    "    1. \"Oil prices soar to all-time record\", \n",
    "    2. \"Stocks end up near year end\", \n",
    "    3. \"Money funds rose in latest week\",\n",
    "    4. \"Stocks up; traders eye crude oil prices\",\n",
    "    5. \"Dollar rising broadly on record trade gain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil</th>\n",
       "      <th>prices</th>\n",
       "      <th>soar</th>\n",
       "      <th>all-time</th>\n",
       "      <th>record</th>\n",
       "      <th>stocks</th>\n",
       "      <th>end</th>\n",
       "      <th>near</th>\n",
       "      <th>year</th>\n",
       "      <th>money</th>\n",
       "      <th>...</th>\n",
       "      <th>latest</th>\n",
       "      <th>week</th>\n",
       "      <th>traders</th>\n",
       "      <th>eye</th>\n",
       "      <th>crude</th>\n",
       "      <th>dollar</th>\n",
       "      <th>rising</th>\n",
       "      <th>broadly</th>\n",
       "      <th>trade</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   oil  prices  soar  all-time  record  stocks  end  near  year  money  ...  \\\n",
       "0 0.41    0.41  0.50      0.50    0.41    0.00 0.00  0.00  0.00   0.00  ...   \n",
       "1 0.00    0.00  0.00      0.00    0.00    0.31 0.78  0.39  0.39   0.00  ...   \n",
       "2 0.00    0.00  0.00      0.00    0.00    0.00 0.00  0.00  0.00   0.45  ...   \n",
       "3 0.36    0.36  0.00      0.00    0.00    0.36 0.00  0.00  0.00   0.00  ...   \n",
       "4 0.00    0.00  0.00      0.00    0.34    0.00 0.00  0.00  0.00   0.00  ...   \n",
       "\n",
       "   latest  week  traders  eye  crude  dollar  rising  broadly  trade  gain  \n",
       "0    0.00  0.00     0.00 0.00   0.00    0.00    0.00     0.00   0.00  0.00  \n",
       "1    0.00  0.00     0.00 0.00   0.00    0.00    0.00     0.00   0.00  0.00  \n",
       "2    0.45  0.45     0.00 0.00   0.00    0.00    0.00     0.00   0.00  0.00  \n",
       "3    0.00  0.00     0.45 0.45   0.45    0.00    0.00     0.00   0.00  0.00  \n",
       "4    0.00  0.00     0.00 0.00   0.00    0.42    0.42     0.42   0.42  0.42  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For better visualization, let's make the tf-idf array a dataframe\n",
    "pd.options.display.float_format = '{:,.2f}'.format # set format for float\n",
    "\n",
    "pd.DataFrame(smoothed_tf_idf, columns = dtm.columns)\n",
    "# the dtm dataframe we created in Step 3 has each word as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  0],\n",
       "       [ 6,  7,  8],\n",
       "       [10, 13, 12],\n",
       "       [16, 15, 14],\n",
       "       [21, 19, 18]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soar', 'all-time', 'oil']\n",
      "['end', 'near', 'year']\n",
      "['funds', 'week', 'latest']\n",
      "['crude', 'eye', 'traders']\n",
      "['gain', 'broadly', 'rising']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7.2. Find the top three words \n",
    "# of each document by TF-IDF weight\n",
    "\n",
    "top=smoothed_tf_idf.argsort()[:,::-1][:,0:3]\n",
    "top\n",
    "\n",
    "for row in top:\n",
    "    print([dtm.columns[x] for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. What to do with TF-IDF\n",
    "- This is the `feature sapce` of text mining (a.k.a. `Bag of Words` or `Vector Space Model`)\n",
    "- Identify important words in each document\n",
    "- Find similar documents\n",
    "    * How to measure simialrity (or distance)? \n",
    "        - `Euclidean distance`\n",
    "        - `Cosine distance`\n",
    "    * Euclidean distance:\n",
    "        - It can be **large** for vectors of high dimension\n",
    "        - `Curse of dimensionality`: In a high-dimensional space, the ratio between the nearest and farthest points approaches 1, i.e. the points essentially become uniformly distant from each other. (https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "    * Cosine similarity: The similarity between two documents is a function of the angle between their vectors in the if-idf vector space. \n",
    "      <img src='cosine.png' width=50% />\n",
    "      <img src='cosine_formula.svg' width=50% />\n",
    "      - Example: A=[0,2,1], B=[1,1,2], then\n",
    "      $$cosine(A,B)=\\frac{0*1+2*1+1*2}{\\sqrt{0+4+1}*\\sqrt{1+1+4}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , 0.26, 0.12],\n",
       "       [0.  , 1.  , 0.  , 0.1 , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.26, 0.1 , 0.  , 1.  , 0.  ],\n",
       "       [0.12, 0.  , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Oil prices soar to all-time record\n",
      "1 Stocks end up near year end\n",
      "2 Money funds rose in latest week\n",
      "3 Stocks up; traders eye crude oil prices\n",
      "4 Dollar rising broadly on record trade gain\n"
     ]
    }
   ],
   "source": [
    "## Exercise 7.4.1 Document similarity\n",
    "\n",
    "# package to calculate distance\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# similarity is 1-distance\n",
    "similarity=1-pairwise_distances(tf_idf, metric = 'cosine')\n",
    "similarity\n",
    "\n",
    "# find top doc similar to the first one\n",
    "# Note the diagonal value is 1, which is the largest\n",
    "\n",
    "np.argsort(similarity)[:,::-1][0,0:2]\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(idx,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Put Everyting together -- Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "# numpy is the package for matrix cacluation\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Step 1. get tokens of each document as list\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, stemming, \n",
    "    # or lemmatization here\n",
    "    \n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "def tfidf(docs):\n",
    "    # step 2. process all documents to get list of token list\n",
    "    docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1, keepdims=True)\n",
    "    tf=np.divide(tf, doc_len)\n",
    "    \n",
    "    # step 5. get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
